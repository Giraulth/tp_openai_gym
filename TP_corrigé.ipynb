{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : RL OpenAI Gym\n",
    "\n",
    "## Objectifs du TP\n",
    "\n",
    "- Découvrir les propriétés de l'environnement du problème du Taxi sur Gym\n",
    "- Implémenter l’algorithme du Q-Learning\n",
    "- Essayer différents paramètres pour l’équation de Bellman\n",
    "- Exécuter l’algorithme du Q-Learning sur un autre environnement : MountainCar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python et importation des bibliothèques utilisées pour le TP\n",
    "\n",
    "Pour ce TP, Python 3 est utilisé ainsi que plusieurs bibliothèques qu'il faut charger :\n",
    "1. **gym** : permet de développer et de comparer des algorithmes d'apprentissage par renforcement.\n",
    "- Documentation : https://gym.openai.com/docs/\n",
    "- Tous les environnements : https://gym.openai.com/envs/#classic_control\n",
    "1. **numpy** : permet de manipuler efficacement des grandes matrices  \n",
    "http://www.numpy.org\n",
    "1. **matplotlib** : permet de tracer et visualiser des données sous formes de graphiques\n",
    "https://matplotlib.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Si vous utilisez un PC du département, lancez le notebook avec Jupyter (commande : jupyter notebook) et installez les modules suivants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\python\\python38\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: scipy in c:\\python\\python38\\lib\\site-packages (from gym) (1.4.1)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\python\\python38\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\python\\python38\\lib\\site-packages (from gym) (1.20.3+vanilla)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\python\\python38\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: future in c:\\python\\python38\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\python\\python38\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\python\\python38\\lib\\site-packages (from matplotlib) (1.20.3+vanilla)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python\\python38\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\python\\python38\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python\\python38\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\python\\python38\\lib\\site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\python\\python38\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\python\\python38\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyglet in c:\\python\\python38\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: future in c:\\python\\python38\\lib\\site-packages (from pyglet) (0.18.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install matplotlib\n",
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Important** : Pour éviter de faire planter le notebook, ne pas oublier de faire un env.close() à la fin de chaque cellule où vous affichez l'environnement avec env.render()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des modules nécessaires\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **A. Taxi**\n",
    "\n",
    "Commencons par reprendre l'exemple du Taxi vu en cours pour découvrir Gym et implémenter l'algorithme du Q-Learning.\n",
    "\n",
    "A titre d'information, voici le code source de l'environnement : https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'environnement\n",
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Découverte de l'environnement**\n",
    "\n",
    "Dans un premier temps, l'objectif va être de se familiariser avec les attributs et méthodes de Gym pour retrouver les propriétés de l'environnement Taxi-v3 énoncées dans le cours. Ces informations permettront de faciliter par la suite la compréhension et l'implémentation de l'algorithme du Q-Learning.\n",
    "\n",
    "Pour obtenir une description de l'environnement, utiliser **help(env.unwrapped)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Question 1** : Afficher l'environnement de Taxi.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Question 2** : Quelles récompenses l'agent peut-il obtenir pour une action ? Quelle est la récompense totale minimale à la fin d'une partie ? maximale si l'agent doit juste déposer le passager ?</font>\n",
    "\n",
    "Pour information, une partie se termine au bout de 200 étapes si le passager n'a pas été déposé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max : 20  \n",
    "  \n",
    "Min : -2000  \n",
    "Mouvement illégal à chaque action avec un maximum de 200 steps avant que la partie soit terminée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Question 3** : Retrouver les informations sur l'espace d'action présentées en cours (discret, 6 actions et associer à chaque valeur l'action réelle associée). </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "# Explore the environment\n",
    "print('Action space:', env.action_space)\n",
    "# Discrete(3)\n",
    "\n",
    "#  |  There are 6 discrete deterministic actions:\n",
    "#  |  - 0: move south\n",
    "#  |  - 1: move north\n",
    "#  |  - 2: move east \n",
    "#  |  - 3: move west \n",
    "#  |  - 4: pickup passenger\n",
    "#  |  - 5: dropoff passenger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Question 4** : Retrouver les informations sur l'espace d'observation présentées en cours (discret, 500 états).</font>\n",
    "\n",
    "Pour rappel, il y a 500 états car :\n",
    "- le taxi peut se retrouver à 25 positions différentes (grille de taille 5x5)\n",
    "- 4 positions (R,G,Y,B) indiquant les positions possibles de départ et d'arrivée du passager\n",
    "- 5 positions indiquant les positions possibles du passager avec 4 positions pour le départ et l'arrivée (R,G,B,Y) et une position quand le passager est dans le taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "print('State space:', env.observation_space)\n",
    "# 25 locations of the taxi (5x5 grid)\n",
    "# 4 letters corresponding to the locations\n",
    "# 5 possible passenger locations with the case when the passenger is in the taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant d'implémenter l'algorithme du Q-Learning, nous allons commencer par faire jouer notre agent de manière aléatoire et observer la récompense par partie. Peut-être que sans entraînement, notre agent est capable de prendre puis de déposer le passager à sa destination..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Agent Random**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Question 5** : Générer un agent jouant 2 parties de Taxi de manière random et afficher en sortie la récompense par épisode.</font>  \n",
    "\n",
    "Exemple de sortie :  \n",
    "Reward of the episode 0 : -200.0  \n",
    "Reward of the episode 1 : -200.0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "Reward of the episode 1 : -785\n",
      "Reward of the episode 1 : -668\n"
     ]
    }
   ],
   "source": [
    "# A compléter entièrement\n",
    "\n",
    "rewards = []\n",
    "\n",
    "for episode in range(2):\n",
    "    tot_reward = 0\n",
    "    \n",
    "    env.reset() # Indicate something is missing here\n",
    "    done = False\n",
    "    while not done:\n",
    "        clear_output(wait = True)\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "        observation, reward, done, info = env.step(env.action_space.sample())  # Complete env.action_space.sample()\n",
    "        tot_reward += reward\n",
    "\n",
    "    rewards.append(tot_reward)\n",
    "\n",
    "env.close()\n",
    "\n",
    "for i in range(len(rewards)):\n",
    "    print(\"Reward of the episode\", episode, \":\", rewards[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est très probable que votre agent n'arrive jamais à déposer à prendre votre passager et à le déposer à sa destination. Pour y remédier, passons au Q-Learning.\n",
    "\n",
    "### **3. Q-Learning**\n",
    "\n",
    "Avec le Q-Learning, votre agent devrait apprendre au fur et à mesure des actions qu'il effectue.\n",
    "\n",
    "Pour rappel, le Q-Learning permet de mesurer la qualité d'une combinaison état-action en termes de récompenses. Il le fait à l'aide d'une Q-Table qui est mise à jour après chaque action avec sa ligne correspondant à l'état et sa colonne à l'action. Un épisode se termine après la réalisation d'un ensemble d'actions. A la fin de l'entraînement, la Q-Table suggère la politique idéale.  \n",
    "On dit également du Q-Learning qu'il est un algorithme *off-policy* ce qui signifie que plutôt que de suivre simplement certaines règles de comportement, connues sous le nom de politique, votre agent peut choisir des actions randoms. De plus, le Q-Learning a une approche *model-free* ce qui signifie que l'agent ne peut pas connaître la valeur d'une action avant de la réaliser.\n",
    "  \n",
    "La mise à jour de la Q-Table se fait avec l'équation de Bellman : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/bellman.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons commencer par définir la Q-Table. Les espaces d'observation et d'action étant discret, celle-ci est de la forme Q(nbEtat, nbAction). Pour l'initialisation de la Q-Table, vous pouvez l'initialiser à 0 ou à tout autre valeur car l'algorithme du Q-Learning garantit la convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Question 6** : Définir la Q-Table.</font>\n",
    "\n",
    "Indice : np.zeros  \n",
    "  \n",
    "Exemple : np.zeros((2, 1))  \n",
    "array([[ 0.],  \n",
    "       [ 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Question 7** : Compléter les parties manquantes de l'algorithme du Q-Learning.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(env, Q, alpha_apprentissage, gamma_actualisation, epsilon, min_eps, episodes):\n",
    "\n",
    "    # Initialisation de variables pour suivre les rewards\n",
    "    reward_list = []  \n",
    "    avg_reward_list = []\n",
    "\n",
    "    # Calcul d'un facteur de réduction pour epsilon. Pour rappel, epsilon est utilisé lors du choix de l'action pour trouver un compromis entre l'exploration de nouveaux états ou l'exploitation des connaissances déjà acquises.\n",
    "    # Ce facteur de réduction permet de faire décroître de manière linéaire epsilon au cours du temps afin d'accorder plus de poids aux connaissances acquises dans la Q-Table.\n",
    "    reduction = (epsilon - min_eps)/episodes\n",
    "\n",
    "    for episode in range(episodes):\n",
    "\n",
    "        # Initialisation des paramètres\n",
    "        tot_reward, reward = 0, 0\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "   \n",
    "        while done != True:\n",
    "\n",
    "            # A compléter : afficher l'agent lors des 10 dernières parties de l'entraînement\n",
    "            if episode >= (episodes - 10):\n",
    "                clear_output(wait = True)\n",
    "                env.render()\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            # A compléter : déterminer la prochaine action : Epsilon Greedy Policy (trouver un compromis entre exploration et exploitation de la Q-Table)\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                # action = meilleure action de la Q-Table pour l'état en cours\n",
    "                action = np.argmax(Q[state])\n",
    "            else:\n",
    "                # action = random. On découvre autre chose\n",
    "                action = env.action_space.sample()\n",
    "                \n",
    "                \n",
    "            # Effectuer l'action choisie et récupérer les informations\n",
    "            state2, reward, done, info = env.step(action)\n",
    "\n",
    "            # Mettre à jour la Q-Table en utilisant l'équation de Bellman\n",
    "            Q[state, action] += alpha_apprentissage * (reward + gamma_actualisation * np.max(Q[state2]) - Q[state, action]) \n",
    "               \n",
    "            # Mettre à jour notre récompense totale et l'état\n",
    "            tot_reward += reward      \n",
    "            state = state2        \n",
    "            \n",
    "        \n",
    "        # Réduire l'exploration lors du Epsilon Greedy Policy en réduisant l'epsilon pour profiter davantage des connaissances acquises dans la Q-Table.\n",
    "        # Dans notre cas, on fait réduire epsilon de manière linéaire jusqu'à un minimum spécifié (min_eps) par rapport au nombre total d'épisodes.\n",
    "        if epsilon > min_eps:\n",
    "            epsilon -= reduction\n",
    "\n",
    "        reward_list.append(tot_reward)\n",
    "\n",
    "        if (episode+1) % 100 == 0:\n",
    "            avg_reward = np.mean(reward_list)\n",
    "            avg_reward_list.append(avg_reward)\n",
    "            reward_list = []\n",
    "            print('Episode {} Average Reward: {}'.format(episode+1, avg_reward))\n",
    "\n",
    "    env.close()\n",
    "        \n",
    "    return avg_reward_list, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Question 8** : Exécuter l'algorithme précédent avec la cellule suivante. Un graphique \"rewards.jpg\" devrait avoir été créé. Celui-ci présente l'évolution de la récompense au fil de l'entrainement. Tester l'algorithme de Q-Learning pour différentes valeurs du facteur d'apprentissage et du facteur d'actualisation.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "Episode 5000 Average Reward: 6.9\n"
     ]
    }
   ],
   "source": [
    "# Lancer l'apprentissage\n",
    "rewards, qtable = QLearning(env, Q, alpha_apprentissage=0.2, gamma_actualisation=0.9, epsilon=0.8, min_eps=0.01, episodes=5000)\n",
    "\n",
    "# Générer le graphique des rewards au cours des épisodes (rewards.jpg)\n",
    "plt.plot(100*(np.arange(len(rewards)) + 1), rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs Episodes')\n",
    "plt.savefig('TaxiRewards.jpg')     \n",
    "plt.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.46029208,  1.62232382,  0.45359832,  1.62256045,  2.9140163 ,\n",
       "       -7.37746122])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtable[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalement votre agent est capable de résoudre aisément le problème, félicitations !  \n",
    "  \n",
    "A présent, pour montrer que les environnements standardisés sont un atout de Gym, nous allons changer d'environnement et adapter légèrement notre algorithme de Q-Learning à un nouveau problème plus visuel (et un peu plus complexe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. MountainCar\n",
    "\n",
    "Le problème : \n",
    "- Une voiture est placée sur une piste unidimensionnelle entre deux sommets.\n",
    "- Objectif : gravir le sommet de droite\n",
    "- Problème : le moteur de la voiture n’est pas assez puissant pour gravir le sommet d’un seul coup\n",
    "\n",
    "![](./images/mountaincar.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'environnement\n",
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Découverte de l'environnement**\n",
    "\n",
    "Dans un premier l'objectif va être de comprendre l'environnement de MountainCar afin de faciliter par la suite la modification de l'implémentation de l'algorithme du Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Question 1** : Quand une partie (= épisode) se termine-t-elle d'un point de vue théorique ? (2 cas possibles)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on MountainCarEnv in module gym.envs.classic_control.mountain_car object:\n",
      "\n",
      "class MountainCarEnv(gym.core.Env)\n",
      " |  MountainCarEnv(goal_velocity=0)\n",
      " |  \n",
      " |  Description:\n",
      " |      The agent (a car) is started at the bottom of a valley. For any given\n",
      " |      state the agent may choose to accelerate to the left, right or cease\n",
      " |      any acceleration.\n",
      " |  \n",
      " |  Source:\n",
      " |      The environment appeared first in Andrew Moore's PhD Thesis (1990).\n",
      " |  \n",
      " |  Observation:\n",
      " |      Type: Box(2)\n",
      " |      Num    Observation               Min            Max\n",
      " |      0      Car Position              -1.2           0.6\n",
      " |      1      Car Velocity              -0.07          0.07\n",
      " |  \n",
      " |  Actions:\n",
      " |      Type: Discrete(3)\n",
      " |      Num    Action\n",
      " |      0      Accelerate to the Left\n",
      " |      1      Don't accelerate\n",
      " |      2      Accelerate to the Right\n",
      " |  \n",
      " |      Note: This does not affect the amount of velocity affected by the\n",
      " |      gravitational pull acting on the car.\n",
      " |  \n",
      " |  Reward:\n",
      " |       Reward of 0 is awarded if the agent reached the flag (position = 0.5)\n",
      " |       on top of the mountain.\n",
      " |       Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
      " |  \n",
      " |  Starting State:\n",
      " |       The position of the car is assigned a uniform random value in\n",
      " |       [-0.6 , -0.4].\n",
      " |       The starting velocity of the car is always assigned to 0.\n",
      " |  \n",
      " |  Episode Termination:\n",
      " |       The car position is more than 0.5\n",
      " |       Episode length is greater than 200\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MountainCarEnv\n",
      " |      gym.core.Env\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, goal_velocity=0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  close(self)\n",
      " |      Override close in your subclass to perform any necessary cleanup.\n",
      " |      \n",
      " |      Environments will automatically close() themselves when\n",
      " |      garbage collected or when the program exits.\n",
      " |  \n",
      " |  get_keys_to_action(self)\n",
      " |  \n",
      " |  render(self, mode='human')\n",
      " |      Renders the environment.\n",
      " |      \n",
      " |      The set of supported modes varies per environment. (And some\n",
      " |      environments do not support rendering at all.) By convention,\n",
      " |      if mode is:\n",
      " |      \n",
      " |      - human: render to the current display or terminal and\n",
      " |        return nothing. Usually for human consumption.\n",
      " |      - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
      " |        representing RGB values for an x-by-y pixel image, suitable\n",
      " |        for turning into a video.\n",
      " |      - ansi: Return a string (str) or StringIO.StringIO containing a\n",
      " |        terminal-style text representation. The text can include newlines\n",
      " |        and ANSI escape sequences (e.g. for colors).\n",
      " |      \n",
      " |      Note:\n",
      " |          Make sure that your class's metadata 'render.modes' key includes\n",
      " |            the list of supported modes. It's recommended to call super()\n",
      " |            in implementations to use the functionality of this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (str): the mode to render with\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      class MyEnv(Env):\n",
      " |          metadata = {'render.modes': ['human', 'rgb_array']}\n",
      " |      \n",
      " |          def render(self, mode='human'):\n",
      " |              if mode == 'rgb_array':\n",
      " |                  return np.array(...) # return RGB frame suitable for video\n",
      " |              elif mode == 'human':\n",
      " |                  ... # pop up a window and render\n",
      " |              else:\n",
      " |                  super(MyEnv, self).render(mode=mode) # just raise an exception\n",
      " |  \n",
      " |  reset(self)\n",
      " |      Resets the environment to an initial state and returns an initial\n",
      " |      observation.\n",
      " |      \n",
      " |      Note that this function should not reset the environment's random\n",
      " |      number generator(s); random variables in the environment's state should\n",
      " |      be sampled independently between multiple calls to `reset()`. In other\n",
      " |      words, each call of `reset()` should yield an environment suitable for\n",
      " |      a new episode, independent of previous episodes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): the initial observation.\n",
      " |  \n",
      " |  seed(self, seed=None)\n",
      " |      Sets the seed for this env's random number generator(s).\n",
      " |      \n",
      " |      Note:\n",
      " |          Some environments use multiple pseudorandom number generators.\n",
      " |          We want to capture all such seeds used in order to ensure that\n",
      " |          there aren't accidental correlations between multiple generators.\n",
      " |      \n",
      " |      Returns:\n",
      " |          list<bigint>: Returns the list of seeds used in this env's random\n",
      " |            number generators. The first value in the list should be the\n",
      " |            \"main\" seed, or the value which a reproducer should pass to\n",
      " |            'seed'. Often, the main seed equals the provided 'seed', but\n",
      " |            this won't be true if seed=None, for example.\n",
      " |  \n",
      " |  step(self, action)\n",
      " |      Run one timestep of the environment's dynamics. When end of\n",
      " |      episode is reached, you are responsible for calling `reset()`\n",
      " |      to reset this environment's state.\n",
      " |      \n",
      " |      Accepts an action and returns a tuple (observation, reward, done, info).\n",
      " |      \n",
      " |      Args:\n",
      " |          action (object): an action provided by the agent\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): agent's observation of the current environment\n",
      " |          reward (float) : amount of reward returned after previous action\n",
      " |          done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
      " |          info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gym.core.Env:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __exit__(self, *args)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from gym.core.Env:\n",
      " |  \n",
      " |  unwrapped\n",
      " |      Completely unwrap this env.\n",
      " |      \n",
      " |      Returns:\n",
      " |          gym.Env: The base non-wrapped gym.Env instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gym.core.Env:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from gym.core.Env:\n",
      " |  \n",
      " |  action_space = None\n",
      " |  \n",
      " |  observation_space = None\n",
      " |  \n",
      " |  reward_range = (-inf, inf)\n",
      " |  \n",
      " |  spec = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(env.unwrapped)\n",
    "#  |  Episode Termination:\n",
    "#  |       The car position is more than 0.5\n",
    "#  |       Episode length is greater than 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Question 2** : Quelle est la récompense totale minimale à la fin d'une partie (pire partie) ? maximale (partie parfaite) ?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max : 0  \n",
    "Du premier coup, atteindre le sommet sinon -1 à chaque action  \n",
    "  \n",
    "Min : -200  \n",
    "-1 à chaque action sans atteindre le sommet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Question 3** : L'espace des actions de l'environnement est-il discret ou continu ? Décrire les différentes actions (mathématiquement) et l'action réelle associée à chaque valeur.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "# Explore the environment\n",
    "print('Action space:', env.action_space)\n",
    "# Discrete(3)\n",
    "\n",
    "# The action space comprises three discrete actions, represented by the integers 0, 1 and 2. (push_left, nothing, push_right)  \n",
    "# 0 : left  \n",
    "# 1 : nothing  \n",
    "# 2 : right  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Question 4** : L'espace des observations de l'environnement est-il discret ou continu ? S'il est discret décrire le nombre d'observations et leur signification sinon décrire les bornes inférieures et supérieures. S'il est continu, identifier également à quoi correspondent les composantes des observations.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Box(-1.2000000476837158, 0.6000000238418579, (2,), float32)\n",
      "[-1.2  -0.07]\n",
      "[0.6  0.07]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print('State space:', env.observation_space)\n",
    "# The state space represents a 2-dimensional box. Each observation is a vector of 2 float values.\n",
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)\n",
    "# The first element of the state vector, representing the car's position, can take on any value in the range -1.2 to 0.6.\n",
    "# The second value, representing the car's velocity, can take on any value in the range -0.07 to 0.07.\n",
    "\n",
    "# To find out what the components of the vector are, use help(env.unwrapped) like before\n",
    "print(env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Question 5** : Dans le cadre du Q-Learning, l'espace d'observation pose problème lors de la création d'une Q-Table, pourquoi ?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme du Q-Learning repose sur une Q-table dont la convergence est garantie à condition que chaque paire état-action soit visitée un nombre suffisamment grand de fois. Dans notre cas, nous avons affaire à un espace d'état continu, ce qui signifie qu'il y a une infinité de paires état-action, ce qui rend impossible de satisfaire cette condition et de créer une Q-table de taille finie.  \n",
    "\n",
    "Pour répondre à ce problème, nous pouvons discrétiser l'espace d'état.\n",
    "\n",
    "Une autre alternative consiste à utiliser le Deep Q-Learning. Cette méthode consiste à approximer l'algorithme de la Q-table avec un réseau de neurones. Cette solution est pertinente dans le cas de problèmes complexes notamment avec des espaces continus ou lorsque la taille de la Q-Table est très conséquente. Le Deep Q-Learning ne sera pas vu dans le cadre de ce cours mais constitue une bonne ouverture pour ceux voulant en savoir plus, cette solution étant très utilisée. Par ailleurs, pour un problème aussi simple que le MountainCar, on peut s'en passer et obtenir de bons résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant d'implémenter l'algorithme du Q-Learning, nous allons commencer par faire jouer notre agent de manière aléatoire et observer la récompense par partie. Peut-être que sans entraînement, notre agent est capable de gravir le sommet..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Agent Random**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de voir en pratique une première fois l'environnement de MountainCar, réalisons de nouveaun agent random.\n",
    "\n",
    "<font color=orange>**Question 6** : Générer un agent jouant 3 parties au MountainCar de manière random et afficher en sortie la récompense par épisode.</font>  \n",
    "\n",
    "Exemple de sortie :  \n",
    "Reward of the episode 0 : -200.0  \n",
    "Reward of the episode 1 : -200.0  \n",
    "Reward of the episode 2 : -200.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward of the episode 0 : -200.0\n",
      "Reward of the episode 1 : -200.0\n",
      "Reward of the episode 2 : -200.0\n",
      "Reward of the episode 3 : -200.0\n",
      "Reward of the episode 4 : -200.0\n",
      "Reward of the episode 5 : -200.0\n",
      "Reward of the episode 6 : -200.0\n",
      "Reward of the episode 7 : -200.0\n",
      "Reward of the episode 8 : -200.0\n",
      "Reward of the episode 9 : -200.0\n"
     ]
    }
   ],
   "source": [
    "# A compléter entièrement\n",
    "\n",
    "for episode in range(3):\n",
    "    tot_reward = 0\n",
    "    env.reset() # Indicate something is missing here\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        observation, reward, done, info = env.step(env.action_space.sample())  # Complete env.action_space.sample()\n",
    "        tot_reward += reward\n",
    "    print(\"Reward of the episode\", episode, \":\", tot_reward)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalement, votre agent ne devrait jamais réussir à remporter une partie. Pour y remédier, passons au Q-Learning.\n",
    "\n",
    "### **3. Q-Learning**\n",
    "\n",
    "Cette fois-ci, votre agent devrait apprendre au fur et à mesure des actions qu'il effectue. Attention cependant, l'algorithme de Q-learning converge mais cela peut nécessiter de nombreuses parties d'entraînement.\n",
    "\n",
    "Pour rappel, le Q-Learning permet de mesurer la qualité d'une combinaison état-action en termes de récompenses. Il le fait à l'aide d'une Q-Table qui est mise à jour après chaque action avec sa ligne correspondant à l'état et sa colonne à l'action. Un épisode se termine après la réalisation d'un ensemble d'actions. A la fin de l'entraînement, la Q-Table suggère la politique idéale.\n",
    "  \n",
    "La mise à jour de la Q-Table se fait avec l'équation de Bellman : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/bellman.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de générer la Q-Table, nous allons devoir discrétiser l'espace d'observation. La Q-Table correspondant à l'ensemble des paires états-actions possibles, nous devons déterminer le nombre d'états possibles pour créer la table aux bonnes dimensions. Notre espace d'observation ayant 2 composantes vectorielles, notre Q-Table sera de la forme Q(s1, s2, a). Comment connaître la taille de s1 et s2 ?  \n",
    "  \n",
    "L'espace d'observation ayant pour limite inférieure (min1 = -1.2, min2 = -0.07) et pour limite supérieure (max1 = 0.6, max2 = 0.07), une manière simple de discrétiser consiste à arrondir le premier élément du vector d'état à 0.1 près et le second élément à 0.01.  \n",
    "Exemple : \n",
    "- Pour la première composante dont les limites sont (min = -1.2 et max = 0.6), les valeurs possibles sont les suivantes : -1.2, -1.1, -1.0, ..., 0.5, 0.6. Nous obtenons ainsi N valeurs (à déterminer à la question suivante).\n",
    "\n",
    "<font color=orange>**Question 7** : Déterminer pour chacune des composantes de l'espace d'observation, le nombre de valeurs possibles après discrétisation.</font>\n",
    "\n",
    "Indices : env.observation_space.high, env.observation_space.low, np.array([10,100]), np.round, +1  et n'oubliez pas de compter les bornes comme des états.  \n",
    "En cas de difficultés, vous pouvez déterminer ces 2 valeurs à la main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6  0.07] [-1.2  -0.07]\n",
      "[18.00000072 14.00000006]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine size of discretized state space\n",
    "print(env.observation_space.high, env.observation_space.low)\n",
    "num_states = (env.observation_space.high - env.observation_space.low)*np.array([10,100])\n",
    "print(num_states)\n",
    "# [18.00000072 14.00000006]\n",
    "num_states = np.round(num_states, decimals=0).astype(int) + 1\n",
    "# [19 15]\n",
    "num_states\n",
    "\n",
    "# OR\n",
    "\n",
    "counter = 0\n",
    "np.arange(-1.2,0.7,0.1).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Question 8** : Définir la Q-Table (format 3D possible).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 15, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, size Q-table : (19x15) x 3 = 855  \n",
    "# 19 : from -1.2 to 0.6  \n",
    "# 15 : from -0.07 to 0.07  \n",
    "# So 19x15 possible pairs  \n",
    "  \n",
    "# Q-Learning  \n",
    "# Not Q(s ,a) but Q(s1, s2, a) because we are dealing with a two-dimensional state space  \n",
    "  \n",
    "# Initial state of our environment. Only the velocity is fixed (=0)\n",
    "\n",
    "Q = np.zeros((num_states[0], num_states[1], env.action_space.n))\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois le nombre de valeurs possibles par composante de l'espace d'observation obtenu, il est nécessaire de faire correspondre les valeurs de l'espace d'observation aux indices de notre table.  \n",
    "Exemple :   \n",
    "- Pour la première composante :   \n",
    "![](./images/mapping_discretization.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Question 9** : Déterminer pour un état donné sa correspondance dans la Q-Table après discrétisation. Pour essayer votre code, vous pourrez prendre comme état **state = env.reset()**. state est dans ce cas de la forme (X, 0) avec X variable. En sortie la deuxième composante de state après discrétisation devrait être 7.</font>\n",
    "\n",
    "Exemple :\n",
    "- state = env.reset() peut retourner [-0.40525229 0.]. La première composante est variable et la seconde est toujours nulle.\n",
    "- votre script devra retourner [8 7] dans ce cas. Ici -0.405... correespond à l'état 8 de la position. La vitesse (2ème composante) sera toujours 7 car initialement la vitesse est nulle.\n",
    "\n",
    "Ce morceau de code sera à réutiliser dans l'algorithme du Q-Learning.  \n",
    "  \n",
    "Indice : np.array([10, 100]), np.round(..., decimals=0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.49713689  0.        ]\n",
      "[7 7]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(state)\n",
    "\n",
    "state_discretized = (state - env.observation_space.low)*np.array([10, 100])\n",
    "state_discretized = np.round(state_discretized, 0).astype(int)\n",
    "\n",
    "print(state_discretized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Question 10** : Compléter les parties manquantes de l'algorithme du Q-Learning.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(env, Q, alpha_apprentissage, gamma_actualisation, epsilon, min_eps, episodes):\n",
    "\n",
    "    # Initialisation de variables pour suivre les rewards\n",
    "    reward_list = []  \n",
    "    avg_reward_list = []\n",
    "\n",
    "    # Calcul d'un facteur de réduction pour epsilon. Pour rappel, epsilon est utilisé lors du choix de l'action pour trouver un compromis entre l'exploration de nouveaux états ou l'exploitation des connaissances déjà acquises.\n",
    "    # Ce facteur de réduction permet de faire décroître de manière linéaire epsilon au cours du temps afin d'accorder plus de poids aux connaissances acquises dans la Q-Table.\n",
    "    reduction = (epsilon - min_eps)/episodes\n",
    "\n",
    "    for episode in range(episodes):\n",
    "\n",
    "        # Initialisation des paramètres\n",
    "        tot_reward, reward = 0, 0\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # A compléter : discrétisation de l'état en cours. Réutiliser la cellule de la question précédente.\n",
    "        state_discretized = (state - env.observation_space.low)*np.array([10, 100])\n",
    "        state_discretized = np.round(state_discretized, 0).astype(int)\n",
    "   \n",
    "        while done != True:\n",
    "\n",
    "            # A compléter : afficher l'agent lors des 20 dernières parties de l'entraînement\n",
    "            if episode >= (episodes - 20):\n",
    "                env.render()\n",
    "\n",
    "            # A compléter : déterminer la prochaine action avec Epsilon Greedy Policy (trouver un compromis entre exploration et exploitation de la Q-Table)\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                # action = meilleure action de la Q-Table pour l'état en cours\n",
    "                action = np.argmax(Q[state_discretized[0], state_discretized[1]])\n",
    "            else:\n",
    "                # action = random. On découvre autre chose\n",
    "                action = env.action_space.sample()\n",
    "                \n",
    "                \n",
    "            # Effectuer l'action choisie et récupérer les informations\n",
    "            state2, reward, done, info = env.step(action)\n",
    "\n",
    "            # Discrétiser le nouvel état obtenu\n",
    "            state2_discretized = (state2 - env.observation_space.low)*np.array([10, 100])\n",
    "            state2_discretized = np.round(state2_discretized, 0).astype(int)\n",
    "\n",
    "            # Si l'action fait gagner l'agent, mettre directement à jour la Q-Table avec la récompense\n",
    "            if done and state2[0] >= 0.5:\n",
    "                Q[state_discretized[0], state_discretized[1], action] = reward\n",
    "            else:\n",
    "                # Mettre à jour la Q-Table en utilisant l'équation de Bellman\n",
    "                Q[state_discretized[0], state_discretized[1], action] += alpha_apprentissage * (reward + gamma_actualisation * np.max(Q[state2_discretized[0], state2_discretized[1]]) - Q[state_discretized[0], state_discretized[1], action])\n",
    "\n",
    "            # Mettre à jour notre récompense totale et l'état\n",
    "            tot_reward += reward      \n",
    "            state_discretized = state2_discretized        \n",
    "            \n",
    "        \n",
    "        # Réduire l'exploration lors du Epsilon Greedy Policy en réduisant l'epsilon pour profiter davantage des connaissances acquises dans la Q-Table.\n",
    "        # Dans notre cas, on fait réduire epsilon de manière linéaire jusqu'à un minimum spécifié (min_eps) par rapport au nombre total d'épisodes.\n",
    "        if epsilon > min_eps:\n",
    "            epsilon -= reduction\n",
    "\n",
    "        reward_list.append(tot_reward)\n",
    "\n",
    "        if (episode+1) % 100 == 0:\n",
    "            avg_reward = np.mean(reward_list)\n",
    "            avg_reward_list.append(avg_reward)\n",
    "            reward_list = []\n",
    "            print('Episode {} Average Reward: {}'.format(episode+1, avg_reward))\n",
    "\n",
    "    env.close()\n",
    "        \n",
    "    return avg_reward_list, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Question 11** : Exécuter l'algorithme précédent avec la cellule suivante. Un graphique \"rewards.jpg\" devrait avoir été créé. Celui-ci présente l'évolution de la récompense au fil de l'entrainement. Tester l'algorithme de Q-Learning pour différentes valeurs du facteur d'apprentissage et du facteur d'actualisation.</font>\n",
    "\n",
    "Cet apprentissage étant plus long que celui de Taxi, on peut sauvegarder la Q-Table. Ainsi si votre notebook plante ou pour une autre raison vous n'aurez pas à relancer l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 Average Reward: -200.0\n",
      "Episode 200 Average Reward: -200.0\n",
      "Episode 300 Average Reward: -200.0\n",
      "Episode 400 Average Reward: -200.0\n",
      "Episode 500 Average Reward: -200.0\n",
      "Episode 600 Average Reward: -200.0\n",
      "Episode 700 Average Reward: -200.0\n",
      "Episode 800 Average Reward: -200.0\n",
      "Episode 900 Average Reward: -200.0\n",
      "Episode 1000 Average Reward: -200.0\n",
      "Episode 1100 Average Reward: -200.0\n",
      "Episode 1200 Average Reward: -200.0\n",
      "Episode 1300 Average Reward: -200.0\n",
      "Episode 1400 Average Reward: -200.0\n",
      "Episode 1500 Average Reward: -200.0\n",
      "Episode 1600 Average Reward: -200.0\n",
      "Episode 1700 Average Reward: -200.0\n",
      "Episode 1800 Average Reward: -200.0\n",
      "Episode 1900 Average Reward: -200.0\n",
      "Episode 2000 Average Reward: -200.0\n",
      "Episode 2100 Average Reward: -200.0\n",
      "Episode 2200 Average Reward: -200.0\n",
      "Episode 2300 Average Reward: -200.0\n",
      "Episode 2400 Average Reward: -200.0\n",
      "Episode 2500 Average Reward: -200.0\n",
      "Episode 2600 Average Reward: -200.0\n",
      "Episode 2700 Average Reward: -200.0\n",
      "Episode 2800 Average Reward: -199.08\n",
      "Episode 2900 Average Reward: -200.0\n",
      "Episode 3000 Average Reward: -199.56\n",
      "Episode 3100 Average Reward: -200.0\n",
      "Episode 3200 Average Reward: -200.0\n",
      "Episode 3300 Average Reward: -200.0\n",
      "Episode 3400 Average Reward: -199.91\n",
      "Episode 3500 Average Reward: -200.0\n",
      "Episode 3600 Average Reward: -198.61\n",
      "Episode 3700 Average Reward: -198.69\n",
      "Episode 3800 Average Reward: -198.97\n",
      "Episode 3900 Average Reward: -197.64\n",
      "Episode 4000 Average Reward: -199.17\n",
      "Episode 4100 Average Reward: -196.2\n",
      "Episode 4200 Average Reward: -196.14\n",
      "Episode 4300 Average Reward: -189.21\n",
      "Episode 4400 Average Reward: -193.67\n",
      "Episode 4500 Average Reward: -188.88\n",
      "Episode 4600 Average Reward: -191.13\n",
      "Episode 4700 Average Reward: -180.35\n",
      "Episode 4800 Average Reward: -198.69\n",
      "Episode 4900 Average Reward: -191.42\n",
      "Episode 5000 Average Reward: -182.98\n"
     ]
    }
   ],
   "source": [
    "# Lancer l'apprentissage\n",
    "rewards, qtable = QLearning(env, Q, alpha_apprentissage=0.2, gamma_actualisation=0.9, epsilon=0.8, min_eps=0.01, episodes=5000)\n",
    "qtable = qtable.reshape(285,3)\n",
    "np.savetxt(\"Qtable.csv\", qtable, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer le graphique des rewards au cours des épisodes (rewards.jpg)\n",
    "plt.plot(100*(np.arange(len(rewards)) + 1), rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs Episodes')\n",
    "plt.savefig('rewards.jpg')     \n",
    "plt.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération de la Q-Table et restructuration en 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ouverture du fichier Qtable.csv pour récupérer la table apprise puis reshape en 3d\n",
    "Q = np.loadtxt(\"Qtable.csv\", delimiter=\";\")\n",
    "Q = Q.reshape(19,15,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>**Question 12** : Utiliser la Q-Table pour résoudre le problème.</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compléter\n",
    "# Utilisation de la Qtable pour résoudre le problème\n",
    "done = False\n",
    "state = env.reset()\n",
    "state_adj = state*np.array([10, 100])\n",
    "state_adj = np.round(state_adj, 0).astype(int)\n",
    "\n",
    "\n",
    "while done != True:\n",
    "    \n",
    "    env.render()\n",
    "    action = np.argmax(Q[state_adj[0], state_adj[1]])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "    state_adj = np.round(state_adj, 0).astype(int)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le TP touche à sa fin... Félicitations !  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
